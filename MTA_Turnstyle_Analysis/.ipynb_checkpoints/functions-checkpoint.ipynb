{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python notebook for the necessary analysis functions, all of these have been moved to a functions python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in october 2019 mta data\n",
    "df1 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_191026.txt')\n",
    "df2 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_191019.txt')\n",
    "df3 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_191012.txt')\n",
    "df4 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_191005.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = ['191026', '191019', '191012', '191005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs_add_time(dataframe_date_list):\n",
    "    \"\"\"\n",
    "    feed in a list of turnstyle dataframes dates\n",
    "    returns the combined datafrae with columns for date time and day of week\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for date in dataframe_date_list:\n",
    "        dfs.append(pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt'.format(date)))\n",
    "    \n",
    "        \n",
    "    # concatenate the dataframes into one\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # rename the exits field\n",
    "    df = df.rename(columns={'EXITS                                                               ': 'EXITS'})\n",
    "    \n",
    "    # create a new column that combines the day and time into one and makes it a datetime object\n",
    "    df[\"DATE_TIME\"] =  pd.to_datetime(df[\"DATE\"] +\" \"+ df[\"TIME\"])\n",
    "    \n",
    "    # add in a day of the week column\n",
    "    df[\"DAY_INT\"] = df[\"DATE_TIME\"].dt.dayofweek\n",
    "    \n",
    "    # create a mapper to map the day of the week nubers to actual string values\n",
    "    day_dict = {\n",
    "        0: 'Monday',\n",
    "        1: 'Tuesday',\n",
    "        2: 'Wednesday',\n",
    "        3: 'Thursday',\n",
    "        4: 'Friday',\n",
    "        5: 'Saturday',\n",
    "        6: 'Sunday'\n",
    "    }\n",
    "    \n",
    "    # add that day of the week string column\n",
    "    df[\"DAY_STR\"] = df[\"DAY_INT\"].map(day_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combine_dfs_add_time(dataframes)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry_and_exit_differences(df):\n",
    "    \"\"\"\n",
    "    Takes in the dataframe with the date time column\n",
    "    returns a data frame with a entry and exit diff column\n",
    "    these new columns tell us how many people exuted/entered in that time period\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort the dataframe by turnstyle date\n",
    "    ordered_date_df = df.sort_values(by=['STATION', 'SCP','UNIT','C/A', 'DATE_TIME'])\n",
    "    \n",
    "    \"\"\"\n",
    "    group by station, scp, unit, and c/a to get the individual counters \n",
    "    then take the difference in entries to get entry changes on each timestamp\n",
    "    \"\"\"\n",
    "    ordered_date_df['ENTRIES_DIFF']=ordered_date_df.groupby(['STATION', 'SCP','UNIT','C/A'])['ENTRIES'].diff().fillna(0)\n",
    "    \n",
    "    \"\"\"\n",
    "    group by station, scp, unit, and c/a to get the individual counters \n",
    "    then take the difference in exits to get exit changes on each timestamp\n",
    "    \"\"\"\n",
    "    ordered_date_df['EXIT_DIFF']=ordered_date_df.groupby(['STATION', 'SCP', 'UNIT', 'C/A'])['EXITS'].diff().fillna(0)\n",
    "    \n",
    "    return ordered_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = add_entry_and_exit_differences(combined_df)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entry_exit_values(df, max_val, min_val=0):\n",
    "    \"\"\"\n",
    "    takes in a dataframe with the entry/exit diff columns and a max and min val for the entry diff\n",
    "    returns a dataframe with the crazy values removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # create mask to remove negative entries and exits or astronomically high differences\n",
    "    pre_cleaning_rows = df.shape[0]\n",
    "    cleaning_mask = (df[\"ENTRIES_DIFF\"]>=min_val) & \\\n",
    "                    (df[\"EXIT_DIFF\"]>=min_val) & \\\n",
    "                    (df[\"ENTRIES_DIFF\"]<max_val) & \\\n",
    "                    (df[\"EXIT_DIFF\"]<max_val)\n",
    "    \n",
    "    df = df[cleaning_mask]\n",
    "    post_cleaning_rows = df.shape[0]\n",
    "    print(\"You removed {} rows in the cleaning\".format(pre_cleaning_rows-post_cleaning_rows))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=clean_entry_exit_values(combined_df, 100000)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totals_combined_per_station(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes in a dataframe with the entry/exit diff columns\n",
    "    returns a dataframe with rows of avg entries, exits, and cobined for each station in desc order\n",
    "    \"\"\"\n",
    "    \n",
    "    # show the total entries and exits, it looks much better\n",
    "    entries_exit_totals = df.groupby([\"STATION\"])[[\"ENTRIES_DIFF\", \"EXIT_DIFF\"]].sum()\n",
    "    \n",
    "    # cobine the entries and exits and sort to get the most popuklar stations\n",
    "    entries_exit_totals[\"COMBINED\"] = entries_exit_totals[\"ENTRIES_DIFF\"] + entries_exit_totals[\"EXIT_DIFF\"]\n",
    "    entries_exit_totals = entries_exit_totals.sort_values(by=[\"COMBINED\"], ascending=False)\n",
    "    \n",
    "    return entries_exit_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_totals = totals_combined_per_station(combined_df)\n",
    "daily_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_combined_per_station(df):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in a dataframe with the entry/exit diff columns\n",
    "    returns a dataframe with rows of total entries, exits, and cobined for each station in desc order\n",
    "    \"\"\"\n",
    "\n",
    "    # show the total entries and exits, it looks much better\n",
    "    entries_exit_totals = df.groupby([\"STATION\", \"DATE\"])[[\"ENTRIES_DIFF\", \"EXIT_DIFF\"]].sum()\n",
    "    entries_exit_avg = entries_exit_totals.groupby([\"STATION\"])[[\"ENTRIES_DIFF\", \"EXIT_DIFF\"]].mean()\n",
    "\n",
    "    # cobine the entries and exits and sort to get the most popuklar stations\n",
    "    entries_exit_avg[\"COMBINED\"] = entries_exit_avg[\"ENTRIES_DIFF\"] + entries_exit_avg[\"EXIT_DIFF\"]\n",
    "    entries_exit_avg = entries_exit_avg.sort_values(by=[\"COMBINED\"], ascending=False)\n",
    "\n",
    "    return entries_exit_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_avgs = avg_combined_per_station(combined_df)\n",
    "daily_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_per_day_of_week(df):\n",
    "    \"\"\"\n",
    "    takes in a dataframe with the entry/exit diff columns\n",
    "    returns a dataframe with rows of total entries, exits, and cobined for the avg traffic \n",
    "    on each DAY OF WEEK for each station\n",
    "    \n",
    "    i.e. Station A on Monday\n",
    "    \"\"\"\n",
    "    \n",
    "    # return the avg usage per day of week per station on each day\n",
    "    total_daily_per_station = df.groupby(['STATION', 'DATE', 'DAY_STR', 'DAY_INT'])[\"ENTRIES_DIFF\", \"EXIT_DIFF\"].sum()\n",
    "\n",
    "    # average out the traffic at each station grouped by day of the week \n",
    "    avg_daily_per_station = total_daily_per_station.groupby([\"STATION\", \"DAY_STR\", 'DAY_INT'])[\"ENTRIES_DIFF\", \"EXIT_DIFF\"].mean()\n",
    "\n",
    "    # cobine the entries and exits and sort to get the most popuklar days at what stations \n",
    "    avg_daily_per_station[\"COMBINED\"] = avg_daily_per_station[\"ENTRIES_DIFF\"] + avg_daily_per_station[\"EXIT_DIFF\"]\n",
    "    avg_daily_per_station.sort_values(by=[\"COMBINED\"], ascending=False)\n",
    "    \n",
    "    return avg_daily_per_station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_per_day = avg_per_day_of_week(combined_df)\n",
    "avg_per_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_per_day_of_week_and_time(df):\n",
    "    \"\"\"\n",
    "    takes in a dataframe with the entry/exit diff columns\n",
    "    returns a dataframe with rows of total entries, exits, and cobined for the avg traffic \n",
    "    on each DAY OF WEEK and TIME SLOT for each station\n",
    "    \n",
    "    i.e. Station A on Monday between 2-6 pm\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the total traffic for each station at each hour of each day\n",
    "    total_hourly_per_station = df.groupby(['STATION', 'DATE', 'DAY_STR', 'DAY_INT' ,'TIME'])[\"ENTRIES_DIFF\", \"EXIT_DIFF\"].sum()\n",
    "\n",
    "    # average out the traffic at each station grouped by day of the week and time slot\n",
    "    avg_hourly_per_station = total_hourly_per_station.groupby([\"STATION\", 'DAY_STR', 'DAY_INT' ,\"TIME\"])[\"ENTRIES_DIFF\", \"EXIT_DIFF\"].mean()\n",
    "\n",
    "    # cobine the entries and exits and sort to get the most popular days and times at each stations \n",
    "    avg_hourly_per_station[\"COMBINED\"] = avg_hourly_per_station[\"ENTRIES_DIFF\"] + avg_hourly_per_station[\"EXIT_DIFF\"]\n",
    "    avg_hourly_per_station.sort_values(by=[\"COMBINED\"], ascending=False).head(50)\n",
    "    \n",
    "    return avg_hourly_per_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_per_day_of_week_and_time = avg_per_day_of_week_and_time(combined_df)\n",
    "avg_per_day_of_week_and_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interested_colored_bar_graph(df, num_stations):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in a dataframe that has combined data and total nscore data\n",
    "    outputs a bar plot with the bars colored by score\n",
    "    \"\"\"\n",
    "    \n",
    "    color_mapper = {\n",
    "        \"7\":'g',\n",
    "        \"6\":'b',\n",
    "        \"5\":'y',\n",
    "        \"4\":'y',\n",
    "        \"3\":'orange',\n",
    "        \"2\":'r',\n",
    "        \"1\":'r',\n",
    "        \"0\":'r',\n",
    "    }\n",
    "    \n",
    "    df[\"color\"] = df[\"total score\"].astype(str).map(color_mapper)\n",
    "    \n",
    "    return(df.head(num_stations).plot.bar(x='STATION', y='COMBINED', color=df[\"color\"], figsize=(10,5)))\n",
    "    plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_stations = pd.read_csv('important_stations.csv')\n",
    "highlighted_stations.drop(['Unnamed: 0'], axis=1, inplace=True, errors='ignore')\n",
    "highlighted_stations.rename(columns={'stations': 'STATION'}, inplace=True)\n",
    "highlighted_stations.head()\n",
    "totals_with_score = pd.merge(daily_totals, highlighted_stations, on='STATION')\n",
    "create_interested_colored_bar_graph(totals_with_score, 50)\n",
    "plt.title(\"Title\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_day_of_week_stacked_bar_graph(df, filtered_station_df):\n",
    "    \"\"\"\n",
    "    Feed it a dataframe with Station, day of week, and traffic info, as well as a list of stations you want to focus on\n",
    "    returns a stacked bar graph of the stations and days of the week traffic, descending order\n",
    "    \"\"\"\n",
    "    reset_df = df.reset_index()\n",
    "    # filter out only the stations that are in our leader stations\n",
    "    leader_daily_avgs = reset_df[reset_df[\"STATION\"].isin(filtered_station_df)]\n",
    "    # find the total traffice per week per station so we can sort by this value\n",
    "    total_in_week = leader_daily_avgs.groupby(\"STATION\")[[\"COMBINED\"]].sum().rename(columns={'COMBINED':'COMBINED_WEEK'})\n",
    "    # merge in that weekly traffic to our df \n",
    "    leader_daily_avgs = pd.merge(leader_daily_avgs, total_in_week, on=\"STATION\")\n",
    "    # sort by weekly traffic so the station with the highest traffic is shown first\n",
    "    df = leader_daily_avgs.sort_values(by=[\"COMBINED_WEEK\"], ascending=False)\n",
    "    \n",
    "    \n",
    "    monday = np.array(df[df[\"DAY_STR\"]==\"Monday\"][\"COMBINED\"])\n",
    "    tuesday = np.array(df[df[\"DAY_STR\"]==\"Tuesday\"][\"COMBINED\"])\n",
    "    wednesday = np.array(df[df[\"DAY_STR\"]==\"Wednesday\"][\"COMBINED\"])\n",
    "    thursday = np.array(df[df[\"DAY_STR\"]==\"Thursday\"][\"COMBINED\"])\n",
    "    friday = np.array(df[df[\"DAY_STR\"]==\"Friday\"][\"COMBINED\"])\n",
    "    saturday = np.array(df[df[\"DAY_STR\"]==\"Saturday\"][\"COMBINED\"])\n",
    "    sunday = np.array(df[df[\"DAY_STR\"]==\"Sunday\"][\"COMBINED\"])\n",
    "    stations = df[df[\"DAY_STR\"]==\"Sunday\"][\"STATION\"]\n",
    "    \n",
    "    plt.bar(stations, sunday, width=0.6, label='sunday', color='#694B36', bottom=saturday+friday+thursday+wednesday+tuesday+monday)\n",
    "    plt.bar(stations, saturday, width=0.6, label='saturday', color='#D67431', bottom=friday+thursday+wednesday+tuesday+monday)\n",
    "    plt.bar(stations, friday, width=0.6, label='friday', color='#752E9C', bottom=thursday+wednesday+tuesday+monday)\n",
    "    plt.bar(stations, thursday, width=0.6, label='thursday', color='#3781CC', bottom=wednesday+tuesday+monday)\n",
    "    plt.bar(stations, wednesday, width=0.6, label='wednesday', color='#E30D45', bottom=tuesday+monday)\n",
    "    plt.bar(stations, tuesday, width=0.6, label='tuesday', color='#ECBE5B', bottom=monday)\n",
    "    plt.bar(stations, monday, width=0.6, label='monday', color='#266931')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show();\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#266931\n",
    "#D67431\n",
    "#694B36"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
