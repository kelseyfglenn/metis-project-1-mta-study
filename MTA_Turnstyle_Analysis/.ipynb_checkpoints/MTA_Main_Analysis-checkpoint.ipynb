{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# import all of the functions we want\n",
    "import functions_python as funcs\n",
    "\n",
    "# You can configure the format of the images: ‘png’, ‘retina’, ‘jpeg’, ‘svg’, ‘pdf’.\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# this statement allows the visuals to render within your Jupyter Notebook\n",
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the max rows to 1000 so we can see a lot of data when we need to\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Import Our Custom Data Set of Stations and Their Corresponding location scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data set was created from seperate analysis we did of the stations locations. This can be found in our [Stations Scoring File](../Station_Scoring_Analysis/Exploration%20and%20focus.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataframe of stations we scored based off of location\n",
    "highlighted_stations = pd.read_csv('important_stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, concat, and clean the turnstyle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in August and september 2019 mta data\n",
    "df_list = ['190803', '190810', '190817', '190824', '190831', '190907', '190914', '190921', '190928']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframe and add a date_time col\n",
    "concat_df = funcs.combine_dfs_add_time(df_list)\n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the time difference\n",
    "concat_df = funcs.add_entry_and_exit_differences(concat_df)\n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are almost two million rows... good thing we are using Pandas!\n",
    "len(concat_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the info before we clean\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "pre_clean_info = concat_df[\"ENTRIES_DIFF\"].describe()\n",
    "pre_clean_info.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the butterfly plot of the pre cleaning data\n",
    "# You can see there are crazy outliers, we need to clean\n",
    "plt.figure(figsize=[10,5])\n",
    "sns.violinplot(data=concat_df[\"ENTRIES_DIFF\"], scale=\"count\", bw=1)\n",
    "plt.ylabel('Entry Count', fontsize = 14)\n",
    "plt.xlabel('Density', fontsize = 14)\n",
    "plt.title(\"Entries Before Cleaning\");\n",
    "# plt.savefig('violin_dirty_data.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 99th percentile of entries after abs value is 1128 so we can use something around there for the filter\n",
    "concat_df[\"ENTRIES_DIFF\"].abs().quantile(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data remove the crazy big numbers (entry/exits > 3000)\n",
    "cleaned_df = funcs.clean_entry_exit_values(concat_df, 3000)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what percent did we get rid of? About 1%\n",
    "(len(concat_df.index) - len(cleaned_df.index) )/ len(concat_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe looks much better, but didnt affect 75 or 50 percentile really, thats good\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "post_clean_info = cleaned_df[\"ENTRIES_DIFF\"].describe()\n",
    "post_clean_info.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot dist also looks good, lets start to analyze!!\n",
    "plt.figure(figsize=[10,5])\n",
    "sns.violinplot(data=cleaned_df[\"ENTRIES_DIFF\"], scale=\"count\", bw=1);\n",
    "plt.ylabel('Entry Count', fontsize = 14)\n",
    "plt.xlabel('Density', fontsize = 14)\n",
    "plt.title(\"Entries After Cleaning\");\n",
    "plt.savefig('violin_cleaned_data.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now We can start analyzing ridership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.02f' % x)\n",
    "# Find the total traffic for the whole data frame at each station\n",
    "totals_per_station = funcs.totals_combined_per_station(cleaned_df)\n",
    "totals_per_station.sort_values(\"COMBINED\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the avg traffic for the whole data frame at each station\n",
    "# WE WILL USE THIS LATER TO DETERMINE WHAT STATIONS TO FOCUS ON!!!\n",
    "avg_per_station = funcs.avg_combined_per_station(cleaned_df)\n",
    "avg_per_station.sort_values(\"COMBINED\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the avg traffic on each day of the week for each station\n",
    "avg_traffic_per_day_of_week = funcs.avg_per_day_of_week(cleaned_df)\n",
    "avg_traffic_per_day_of_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the avg traffic at each time slot on each day for each station\n",
    "avg_traffic_per_day_per_time_per_station = funcs.avg_per_day_of_week_and_time(cleaned_df)\n",
    "avg_traffic_per_day_per_time_per_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now We Look At Our High Scoring Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the highlighted stations\n",
    "highlighted_stations.drop(['Unnamed: 0'], axis=1, inplace=True, errors='ignore')\n",
    "highlighted_stations.rename(columns={'stations': 'STATION'}, inplace=True)\n",
    "highlighted_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the avg daily ridership df\n",
    "avg_per_station_with_score = pd.merge(avg_per_station, highlighted_stations, on='STATION')\n",
    "avg_per_station_with_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average ridership per day with the color being score\n",
    "funcs.create_interested_colored_bar_graph(avg_per_station_with_score, 50)\n",
    "\n",
    "grey_patch = mpatches.Patch(color='#bab9b7', label='Score < 3')\n",
    "blue_patch = mpatches.Patch(color='#1DACD6', label='Score = 3')\n",
    "green_patch = mpatches.Patch(color='#1DB91D', label='Score > 3')\n",
    "\n",
    "\n",
    "plt.legend(handles=[green_patch, blue_patch, grey_patch])\n",
    "plt.ylabel('Average Combined Exits and Entries', fontsize = 14);\n",
    "plt.xlabel('Station', fontsize = 14);\n",
    "plt.title('Average Daily Traffic (Entries and Exits)', family='serif', fontsize = 20)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=70);\n",
    "# plt.savefig('daily_traffic_with_interested_stations.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Lets Figure Out Which Iportant Ones To Focus On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the merged dataframe to keep only the stations with interest score gretater than or = 3\n",
    "avg_per_station_high_scores = avg_per_station_with_score[avg_per_station_with_score[\"total score\"]>=3]\n",
    "# take the top 10, this is what we will focus on\n",
    "focused_stations_df = avg_per_station_high_scores.head(10)\n",
    "# grab the station row (we will use this as our graph index)\n",
    "focused_stations = focused_stations_df[\"STATION\"]\n",
    "# stations we want oprdered by traffic\n",
    "focused_stations_df\n",
    "# expot or focused stations to use elsewhere\n",
    "# focused_stations_df.to_csv('focused_station.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perecent of total traffic through the stations we are focusing on\n",
    "percent_of_total_traffic = focused_stations_df[\"COMBINED\"].sum() / avg_per_station_with_score[\"COMBINED\"].sum()\n",
    "percent_of_total_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of important traffic through the stations we are focusing on\n",
    "percent_of_important_traffic = focused_stations_df[\"COMBINED\"].sum() / avg_per_station_high_scores[\"COMBINED\"].sum()\n",
    "percent_of_important_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a normalized column on our high scoring stations\n",
    "avg_per_station_high_scores[\"NORMALIZED_TRAFFIC\"] = avg_per_station_high_scores[\"COMBINED\"]/avg_per_station_high_scores[\"COMBINED\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the percentage of traffic through each station as it compares to all important stations\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.bar(avg_per_station_high_scores[\"STATION\"], avg_per_station_high_scores[\"NORMALIZED_TRAFFIC\"])\n",
    "plt.ylabel('Percent of Important Traffic', fontsize = 14);\n",
    "plt.xlabel('Station', fontsize = 14);\n",
    "plt.title('Normalized Important Traffic', family='serif', fontsize = 20);\n",
    "plt.text(2,0.099,'Selected Stations \\n70% of Traffic',fontsize = 16,color = 'red');\n",
    "plt.axvline(x=9.5, color='red')\n",
    "plt.xticks(rotation=90);\n",
    "# plt.savefig('normalized_important_traffic.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Lets Look At When To Focus on these Ten Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what days we should focus on at each station... Doesnt seem like anything stands out\n",
    "funcs.create_day_of_week_stacked_bar_graph(avg_traffic_per_day_of_week, focused_stations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at day of week data across all stations, combine days of week\n",
    "focused_mask = avg_traffic_per_day_of_week.reset_index()[\"STATION\"].isin(focused_stations)\n",
    "avg_traffic_per_day_of_week_focused = avg_traffic_per_day_of_week.reset_index()[focused_mask]\n",
    "avg_traffic_per_day_of_week_focused.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add normalized column\n",
    "aggregated_per_day_of_week = avg_traffic_per_day_of_week_focused.groupby([\"DAY_STR\",\"DAY_INT\"])[[\"ENTRIES_DIFF\", \"EXIT_DIFF\", \"COMBINED\"]].sum()\n",
    "aggregated_per_day_of_week[\"COMBINED_NORMALIZED\"] = aggregated_per_day_of_week[\"COMBINED\"] / aggregated_per_day_of_week[\"COMBINED\"].sum()\n",
    "aggregated_per_day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_per_day_of_week = aggregated_per_day_of_week.reset_index().sort_values(by=[\"DAY_INT\"])\n",
    "aggregated_per_day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the normalized traffic on each day of the week, it really just looks like we want to avoid weekends\n",
    "plt.figure(figsize=[8,5])\n",
    "barlist = plt.bar(aggregated_per_day_of_week[\"DAY_STR\"], aggregated_per_day_of_week[\"COMBINED_NORMALIZED\"])\n",
    "barlist[5].set_color('xkcd:dull red')\n",
    "barlist[6].set_color('xkcd:dull red')\n",
    "plt.ylabel('Percent of Weekly Traffic', fontsize = 14);\n",
    "plt.xlabel('Day Of Week', fontsize = 14);\n",
    "plt.title('Normalized Daily Traffic', family='serif', fontsize = 20);\n",
    "# plt.savefig('week_day_normalized_traffic.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets look at time of day\n",
    "focused_time_mask = avg_traffic_per_day_per_time_per_station.reset_index()[\"STATION\"].isin(focused_stations)\n",
    "avg_traffic_per_day_per_time_per_station = avg_traffic_per_day_per_time_per_station.reset_index()[focused_time_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time to time type\n",
    "avg_traffic_per_day_per_time_per_station[\"TIME\"] = pd.to_datetime(avg_traffic_per_day_per_time_per_station[\"TIME\"], format='%H:%M:%S').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime time objects to make buckets\n",
    "midnight = datetime.time(0,1,0,0)\n",
    "four_am = datetime.time(4,0,0,0)\n",
    "eight_am = datetime.time(8,0,0,0)\n",
    "noon = datetime.time(12,0,0,0)\n",
    "four_pm = datetime.time(16,0,0,0)\n",
    "eight_pm = datetime.time(20,0,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these time objects to create masks\n",
    "midnight_mask = avg_traffic_per_day_per_time_per_station[\"TIME\"] < midnight\n",
    "\n",
    "four_mask = (avg_traffic_per_day_per_time_per_station[\"TIME\"] > midnight) & \\\n",
    "(avg_traffic_per_day_per_time_per_station[\"TIME\"] <= four_am)\n",
    "\n",
    "eight_mask = (avg_traffic_per_day_per_time_per_station[\"TIME\"] > four_am) & \\\n",
    "(avg_traffic_per_day_per_time_per_station[\"TIME\"] <= eight_am)\n",
    "\n",
    "noon_mask = (avg_traffic_per_day_per_time_per_station[\"TIME\"] > eight_am) & \\\n",
    "(avg_traffic_per_day_per_time_per_station[\"TIME\"] <= noon)\n",
    "\n",
    "sixteen_mask = (avg_traffic_per_day_per_time_per_station[\"TIME\"] > noon) & \\\n",
    "(avg_traffic_per_day_per_time_per_station[\"TIME\"] <= four_pm)\n",
    "\n",
    "twenty_mask = (avg_traffic_per_day_per_time_per_station[\"TIME\"] > four_pm) & \\\n",
    "(avg_traffic_per_day_per_time_per_station[\"TIME\"] <= eight_pm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket each time stamp and sum up their traffic\n",
    "traffic_list = [\n",
    "    avg_traffic_per_day_per_time_per_station[four_mask][\"COMBINED\"].sum(),\n",
    "    avg_traffic_per_day_per_time_per_station[eight_mask][\"COMBINED\"].sum(),\n",
    "    avg_traffic_per_day_per_time_per_station[noon_mask][\"COMBINED\"].sum(),\n",
    "    avg_traffic_per_day_per_time_per_station[sixteen_mask][\"COMBINED\"].sum(),\n",
    "    avg_traffic_per_day_per_time_per_station[twenty_mask][\"COMBINED\"].sum(),\n",
    "    avg_traffic_per_day_per_time_per_station[midnight_mask][\"COMBINED\"].sum()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a list of avg traffic at each time stamp across our focused stations\n",
    "traffic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize it\n",
    "norm_traffic_list = [x / sum(traffic_list) for x in traffic_list]\n",
    "norm_traffic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "hour_list = [4, 8, 12, 16, 20, 24]\n",
    "plt.bar(hour_list, norm_traffic_list, width=-3.9, align='edge', color=\"xkcd:sage\")\n",
    "ticks_x = np.linspace(0, 24, 7)\n",
    "plt.xticks(ticks_x)\n",
    "plt.ylabel('Percent of Daily Ridership', fontsize = 14);\n",
    "plt.xlabel('Hour', fontsize = 14);\n",
    "plt.title('Normalized Daily Traffic', family='serif', fontsize = 20);\n",
    "plt.xticks(rotation=90)\n",
    "# plt.savefig('normalized_time_of_day.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time data is not perfect. It gives us a loose understanding of the traffic pattern throughout the day. But the timestamps data was very dirty and the varying bucket sizes made it tough to analyze. We would need to do more work to get a better idea of ehat times to visit what stations. We ran out of time there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try To Get a Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We started playing around with heat maps but did not get very far in time. You can ignore\n",
    "\n",
    "locations_df = pd.read_csv('http://web.mta.info/developers/data/nyct/subway/Stations.csv')\n",
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_df.STATION.unique()), len(locations_df[\"Stop Name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df.rename(columns={\"Stop Name\":\"STATION\"}, inplace=True)\n",
    "locations_df[\"STATION\"] = locations_df[\"STATION\"].str.lower()\n",
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_per_station = avg_per_station.reset_index()\n",
    "avg_per_station[\"STATION\"] = avg_per_station[\"STATION\"].str.lower()\n",
    "avg_per_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_with_loc = pd.merge(avg_per_station, locations_df,on=\"STATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_with_loc[\"STATION\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirtyFour = cleaned_df[cleaned_df[\"STATION\"]==\"34 ST-PENN STA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
